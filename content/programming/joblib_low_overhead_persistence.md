Title: Joblib persistence improvements
Date: 2016-05-20
Category: Python
Tags: joblib, persistence, big data
Slug: New low-overhead persistence in joblib for big data
Authors: Alexandre Abadie & GaÃ«l Varoquaux
Profile_image: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/110px-Python-logo-notext.svg.png
Summary: New persistence in joblib enables low-overhead storage of big data contained in arbitrary objects

## Joblib persistence improvements

Joblib is a powerful python package when working on so called __Big Data__, e.g
data that can consume up to the available RAM (several GB nowadays), generally
numpy arrays or arbitrary containers (list, dict) of numpy arrays. In order to
speedup Big Data processing and management, joblib uses a transparent disk
persistence which is based on functions `dump` and `load`. The underlying
mecanism is based on an advanded usage of
[pickle](https://docs.python.org/3/library/pickle.html).

Some [recent work](https://github.com/joblib/joblib/pull/260) on joblib has been
achieved around memory consumption and compression when dealing with data
persistence.

Up to version 0.9.4, it was known that the process of dumping/loading
persisted data with compression was a memory gap, mainly because of memory
copies of numpy arrays, limitating the maximum size of usable data with joblib.

Another drawback was that each numpy array of an arbitrary container were dumped
in separate `.npy` files, increasing the complexity of the cache management on
the file system.

Let's explain those problems with a bit of code:
* Container with several numpy arrays is persisted in multiple files:
```python
>>> import numpy as np
>>> import joblib
>>> joblib.__version__
'0.9.4'
>>> obj = [np.arange(1000000), np.ones((10000, 10000))]

# 3 files are generated:
>>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
['/tmp/test.pkl', '/tmp/test.pkl_01.npy', '/tmp/test.pkl_02.npy']
>>> joblib.load('/tmp/test.pkl')
[array([     0,      1,      2, ..., 999997, 999998, 999999]),
 array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
        ..., 
        [ 1.,  1.,  1., ...,  1.,  1.,  1.]])]
```
* Memory footprint can be profiled using the excellent package
[memory_profiler package](https://pypi.python.org/pypi/memory_profiler) (on
`joblib.dump` and `joblib.load` functions):
```python
# memory_profile.py
import numpy as np
import joblib
import gc

@profile
def generate_data():
    return [np.arange(1000000), np.ones((10000, 10000))]

def main():
    obj = generate_data()
    joblib.dump(obj, '/tmp/test.pkl', compress=True)
    del obj
    gc.collect()
    joblib.load('/tmp/test.pkl')

if __name__ == '__main__':
    main()
```
and in a console, run:
```bash
$ prof run memory_profile.py && mprof plot
```
![Memory profiler]({filename}attachments/old_pickle_mem_profile.png)

Let's now discover the new features and improvements that comes
with 0.10.0. After that we'll give some comparison benchmarks and explain how
this is possible.

### What's new

joblib now __persists numpy arrays in a single file__ and with __no memory
copies__ during read and write steps. This __allows contextlib syntaxic sugar
usage__ when playing with cached objects. The other big step forward
is the internal usage of __all compression formats available in the standard
library__.

Early joblib adopters can also be reassured : this new version is __still
compatible with pickles generated by older versions__ (>= 0.8.4). By the way,
you are encourage to update (rebuild?) your cache if you want to take advantage
of this new version.

If we try again the examples above, we can already see improvements:
* All numpy arrays are persisted in a single file:
```python
>>> import numpy as np
>>> import joblib
>>> joblib.__version__
'0.10.0'
>>> obj = [np.arange(1000000), np.ones((10000, 10000))]

# only 1 file is used:
>>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
['/tmp/test.pkl']
>>> joblib.load('/tmp/test.pkl')
[array([     0,      1,      2, ..., 999997, 999998, 999999]),
 array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
        ..., 
        [ 1.,  1.,  1., ...,  1.,  1.,  1.]])]
```
* Memory usage is now stable:
![Memory profiler]({filename}attachments/new_pickle_mem_profile.png)

Let's now play a bit with the new compression methods available in the python
library: __zlib, gzip, bz2, lzma and xz__ (the last 2 ones are available since
version 3.3). An important thing is that the files are compressed using a
__valid standard compression file format__: for instance, regular command line
tools (zip/unzip, gzip/gunzip, bzip2, lzma, xv) can be used to
compress/uncompress a pickled file generated with joblib. __The compressor is
selected automatically when the file name has an explicit extension__:
```python
>>> joblib.dump(obj, '/tmp/test.pkl.z')   # zlib
['/tmp/test.pkl.z']
>>> joblib.dump(obj, '/tmp/test.pkl.gz')  # gzip
['/tmp/test.pkl.gz']
>>> joblib.dump(obj, '/tmp/test.pkl.bz2')  # bz2
['/tmp/test.pkl.bz2']
>>> joblib.dump(obj, '/tmp/test.pkl.lzma')  # lzma
['/tmp/test.pkl.lzma']
>>> joblib.dump(obj, '/tmp/test.pkl.xz')  # xz
['/tmp/test.pkl.xz']
```

Of course, one can play with the compression level, but then the compressor has
to be given explicitly:
```python
>>> joblib.dump(obj, '/tmp/test.pkl.compressed', compress=('zlib', 6))
['/tmp/test.pkl.compressed']
>>> joblib.dump(obj, '/tmp/test.compressed', compress=('lzma', 6))
['/tmp/test.pkl.compressed']
```

Compressed files are valid regarding the compression method used. Joblib uses
the Magic number at the beginning of the file to choose the right decompressor,
making compressed pickle load transparent:
```python
>>> joblib.load('/tmp/test.compressed')
[array([     0,      1,      2, ..., 999997, 999998, 999999]),
 array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
        ..., 
        [ 1.,  1.,  1., ...,  1.,  1.,  1.]])]
```

To conclude on those new exciting features, we can say a few words on file
handles with contextlib. Indeed, all numpy arrays are stored in a
single file, itself using standard compression formats and, as a consequence,
__joblib now takes advantage of python `with` statement__ with objects
implementing the buffer interface:

```python
>>> with open('/tmp/test.pkl', 'wb') as f:
>>>    joblib.dump(obj, f)
['/tmp/test.pkl']
>>> with open('/tmp/test.pkl', 'rb') as f:
>>>    print(joblib.load(f))
[array([     0,      1,      2, ..., 999997, 999998, 999999]),
 array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
        ..., 
        [ 1.,  1.,  1., ...,  1.,  1.,  1.]])]
```
This also works with regular file, like `gzip.GzipFile` (or bz2.Bz2File,
lzma.LzmaFile):
```python
>>> import gzip
>>> with gzip.GzipFile('/tmp/test.pkl.gz', 'wb') as f:
>>>    joblib.dump(data, f)
['/tmp/test.pkl.gz']
>>> with gzip.GzipFile('/tmp/test.pkl.gz', 'rb') as f:
>>>    print(joblib.load(f))
```
Be sure that you use a decompressor matching the internal compression when
loading with the above method. Anyway, if unsure, simply use `open`, joblib will
__select the right decompressor__ for you:
```python
>>> with open('/tmp/test.pkl.gz', 'rb') as f:
>>>     print(joblib.load(f))
[array([     0,      1,      2, ..., 999997, 999998, 999999]),
 array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
        ..., 
        [ 1.,  1.,  1., ...,  1.,  1.,  1.]])]
```

### Memory consumption and performance comparisons

No more memory copies ! The overhead only corresponds to the size of the buffers (~30MB)
used during writing/reading steps.

With large arrays the speed is slightly faster : put nice matplotlib figures here.

Discussion with dict/list : io.BytesIO does copy in memory but is faster
compared to io.BufferedIO : put some profiling here.

### Enhancement strategy

Hack on top of pickle (already there), different strategies tested, use low level numpy primitives
(e.g nditer). No more compatible with

Performance improvements : implemented a custom Zlib file compressor (for zlib and
gzip) + heavy use of buffering.

Side effects : the generated pickle is not compatible with standard library
pickle module.

### Conclusion and future work

Memory copies were a ressource gap when caching on disk very large
numpy arrays, e.g arrays with a size close to the available RAM on the computer.
The solution was to use intensive buffering and a lot of hacking on top of
pickle and numpy.

Pickling numpy arrays using file handle is a first step toward pickling in
sockets. Then it make broadcasting of data possible between computing units on a
network.

Another potential improvements is to make the supported list of compressors
extendable by allowing external project to register new ones. Some work has
already been started with LZO (using python-lzo) but LZ4 also seems to be an
interesting ones.

Thanks to [@lesteve](https://github.com/lesteve), [@ogrisel](https://github.com/ogrisel) and [@GaelVaroquaux](https://github.com/GaelVaroquaux) for the valuable help, reviews
and support.


###Notes

Data: sklearn.datasets.fetch_lfw_people

Strategies:

- pickle
- numpy.savez
- old joblib
  no zlib
  zlib3
- new joblib
  no zlib
  zlib3

Info displayed:
- read time
- write time
- memory footprint
- DU

Other graph:
 - comparison of compressions



