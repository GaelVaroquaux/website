New persistence strategy in joblib
##################################

:date: 2016-05-20
:tags: joblib, persistence, big data
:slug: New low-overhead persistence in joblib for big data
:authors: Alexandre Abadie & GaÃ«l Varoquaux
:profile_image: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/110px-Python-logo-notext.svg.png
:summary: New persistence in joblib enables low-overhead storage of big data contained in arbitrary objects


The problem
===========

Joblib is a powerful Python package when working on so called **Big Data**, e.g
data that can consume up to the available RAM (several GB nowadays), generally
numpy arrays or arbitrary containers (list, dict) of numpy arrays. In order to
speedup Big Data processing and management, joblib uses a transparent disk
persistence which is based on functions ``dump`` and ``load``. The internal
mecanism relies on an advanded usage of `pickle
<https://docs.python.org/3/library/pickle.html>`__.

Some `recent work <https://github.com/joblib/joblib/pull/260>`__ on joblib has been
achieved to solve memory consumption when dealing with data persistence.

Up to version 0.9.4, it was known that the process of dumping/loading
persisted data **with compression** was a memory gap, mainly because of internal
copies of data, limitating the maximum size of usable data with joblib.

Another drawback was that each numpy array (>10MB in memory) of an arbitrary
container was dumped in separate ``.npy`` file, increasing the complexity of
the cache management on the file system.

Let's put in evidence those problems with a bit of code:

* a container of several numpy arrays is persisted in multiple files:

  .. code-block:: python
                 
                  >>> import numpy as np
                  >>> import joblib # joblib version: 0.9.4
                  >>> obj = [np.ones((5000, 5000)), np.random.random((5000, 5000))]
                  
                  # 3 files are generated:
                  >>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
                  ['/tmp/test.pkl', '/tmp/test.pkl_01.npy.z', '/tmp/test.pkl_02.npy.z']
                  >>> joblib.load('/tmp/test.pkl')
                  [array([[ 1.,  1., ...,  1.,  1.]],
                   array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]


* Memory footprint can be profiled using the excellent `memory_profiler
  package <https://pypi.python.org/pypi/memory_profiler>`__ with this
  `gist <https://gist.github.com/aabadie/7cba3385406d1cec7d3dd4407ba3f164>`__:

  .. image:: {filename}attachments/old_pickle_mem_profile.png

  We obviously see the instability of memory usage during the calls to ``dump``
  and ``load`` functions.


Let's now discover the new features and improvements that comes with
version 0.10.0. After that, we'll compare speed and memory consumption with
other libraries and discuss the results. Then we'll give some details about the
new internal implementation.

What's new
==========

joblib now **persists numpy arrays in a single file** and with **no memory
copies** during dump and load steps. This **allows contextlib syntaxic
sugar usage** when playing with cached objects. The other big step forward
is the internal usage of **all compression formats available in the standard
library**.

Early joblib adopters can also be reassured : this new version is **still
compatible with pickles generated by older versions** (>= 0.8.4). By the way,
you are encourage to update (rebuild?) your cache if you want to take advantage
of this new version.

If we try again the examples above, we can already see improvements:
* All numpy arrays are persisted in a single file:

  
.. code-block:: python

                >>> import numpy as np
                >>> import joblib # joblib version: 0.10.0
                >>> obj = [np.ones((5000, 5000)), np.random.random((5000, 5000))]
                
                # only 1 file is generated:
                >>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
                ['/tmp/test.pkl']
                >>> joblib.load('/tmp/test.pkl')
                [array([[ 1.,  1., ...,  1.,  1.]],
                 array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]


          
* Memory usage is now stable:

  .. image:: {filename}attachments/new_pickle_mem_profile.png


We can also play a bit with the new available compression methods based on
python standard library modules: **zlib, gzip, bz2, lzma and xz** (the last 2
ones are available since version 3.3).

An important thing is that the generated compressed files uses a **standard
compression file format**: for instance, regular command line tools (zip/unzip,
gzip/gunzip, bzip2, lzma, xv) can be used to compress/uncompress a pickled file
generated with joblib. Joblib will be able to load cache compressed with those
tools. When dumping data into cache, **the compressor is selected automatically
when the file name has an explicit extension**:




.. code-block:: python
               
                >>> joblib.dump(obj, '/tmp/test.pkl.z')   # zlib
                ['/tmp/test.pkl.z']
                >>> joblib.dump(obj, '/tmp/test.pkl.gz')  # gzip
                ['/tmp/test.pkl.gz']
                >>> joblib.dump(obj, '/tmp/test.pkl.bz2')  # bz2
                ['/tmp/test.pkl.bz2']
                >>> joblib.dump(obj, '/tmp/test.pkl.lzma')  # lzma
                ['/tmp/test.pkl.lzma']
                >>> joblib.dump(obj, '/tmp/test.pkl.xz')  # xz
                ['/tmp/test.pkl.xz']


Of course, one can play with the compression level, but then the compressor has
to be set explicitly:


.. code-block:: python
               
                >>> joblib.dump(obj, '/tmp/test.pkl.compressed', compress=('zlib', 6))
                ['/tmp/test.pkl.compressed']
                >>> joblib.dump(obj, '/tmp/test.compressed', compress=('lzma', 6))
                ['/tmp/test.pkl.compressed']

                
Joblib uses the Magic number of the file to determine the right decompressor,
making compressed pickle load transparent:


.. code-block:: python
               
                >>> joblib.load('/tmp/test.compressed')
                [array([[ 1.,  1., ...,  1.,  1.]],
                 array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]


To conclude on those new exciting features, let's say a few words on file
handles with contextlib. Indeed, all numpy arrays are stored in a
single file, itself using standard compression formats so, as a consequence,
**joblib now takes advantage of python with statement** with file-like
objects. Moreover this opens the door to **storing cache data in database blob
or cloud storage such as Amazon S3, Amazon Glacier and Google Cloud Storage**
(via the great python package `boto <https://github.com/boto/boto>`_).

Here are some example of persisting data using the ``with`` statement:


.. code-block:: python
               
                >>> with open('/tmp/test.pkl', 'wb') as f:
                >>>    joblib.dump(obj, f)
                ['/tmp/test.pkl']
                >>> with open('/tmp/test.pkl', 'rb') as f:
                >>>    print(joblib.load(f))
                [array([[ 1.,  1., ...,  1.,  1.]],
                 array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]


This also works with compression file object available in the standard library,
like ``gzip.GzipFile``, ``bz2.Bz2File`` or ``lzma.LzmaFile``:


.. code-block:: python

                >>> import gzip
                >>> with gzip.GzipFile('/tmp/test.pkl.gz', 'wb') as f:
                >>>    joblib.dump(data, f)
                ['/tmp/test.pkl.gz']
                >>> with gzip.GzipFile('/tmp/test.pkl.gz', 'rb') as f:
                >>>    print(joblib.load(f))


Be sure that you use a decompressor matching the internal compression when
loading with the above method, otherwise python will raise an error. Anyway, if
you are unsure, simply use ``open``, joblib will **select the right decompressor**
for you:


.. code-block:: python

                >>> with open('/tmp/test.pkl.gz', 'rb') as f:
                >>>     print(joblib.load(f))
                [array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],
                ..., 
                [ 1.,  1.,  1., ...,  1.,  1.,  1.]]),
                array([[ 0.47006195,  0.5436392 ,  0.78962947, ...,  0.77567775,
                ..., 
                0.1218267 ,  0.48592789]])]

          
Speed, memory consumption and discussion
========================================


It's now time to have a look at performances. We now have a friendly API but
does it have an impact on them ? The answer is **it depends on the data**.

Joblib philosophy is to have the **minimum dependencies** (only numpy) and to
**be agnostic to the input data**. So joblib's goal is to able to deal with any
kind of data while trying to **be as efficient as possible with numpy arrays**.

To illustrate the benefits and cost of the new persistence implementation, let's
now compare a real life use cases
(`LFW dataset from scikit-learn <http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html>`_)
with different libraries.

The compared libraries are:

* Joblib, tested for raw and compressed (zlib) files, with 2 different versions,
  0.9.4 and master (dev),
* Pickle, tested for raw and compressed (gzip) files,
* Numpy, tested for raw and compressed (zlib) files.

The compression level and underlying method used are the same : zlib (gzip is
based on zlib but adds crc checks) with a compression level of 3.

The following results were generated with this
`gist <https://gist.github.com/aabadie/2ba94d28d68f19f87eb8916a2238a97c>`_:


.. image:: {filename}attachments/persistence_lfw_bench.png

           
The four first lines use non compressed persistence strategies, the last four
use persistence with zlib/gzip strategies.

First, we can put aside the disk space used as the results are as expected : non
compressed files has the same size as the persisted data, compressed files are
smaller.

Regarding the speed, the results between joblib 0.9.4 and 0.10.0 are
similar whereas **numpy and pickle are clearly slower than joblib** in both
compressed and non compressed cases.

Let's now have a look at the memory consumption. Without compression, old and
new joblib versions are the same but with compression, the old joblib version is
clearly worse than the new one.
Again **pickle and moreover numpy are clearly worse than joblib in terms of
memory consumption**. This can be explained by the fact that numpy relies on
pickle if the object is not a pure numpy array (a list or a dict with arrays for
example), so in this case it inherits the memory drawbacks from pickle. When
persisting pure numpy arrays (not tested here), numpy uses its interal save/load
functions which are quite efficient in terms of speed and memory consumption.


Enhancement strategy
====================


It is now time to give some details about the internal implementation of joblib
persistence functions.

First, as we said above, joblib historically relies on pickle python
implementation through Pickler/Unpickler subclasses. This has been sligthly
refactored in the new version as follows:

* When pickling an arbitrary object, if an ``np.ndarray`` object is reached,
  instead of using the default pickling functions (__reduce__()), the joblib
  Pickler replaces in pickle stream the ndarray with a wrapper object containing
  all important array metadata (shape, dtype, flags). Then it writes the array
  content in the pickle file. **Note that this step breaks the pickle
  compatibility**.
* When unpickling a pickle file, when pickle reaches the array wrapper, as the
  object is already fully read in the pickle stream, the file handle is at the
  beginning of the array content. So at this point the Unpickler simply
  reconstruct an array based on the metadata contained in the wrapper and then
  fill the array buffer directly from the file. The object returned is the
  reconstructed array, the array wrapper being dropped.

This technique allows joblib to pickle all objects in a single file but also to
stay efficient in memory consumption during dump and load.

The other main change in the current persitence workflow concerns the
compression strategy. As the pickling refactoring described above opened the door
to file objects usage, joblib is now able to persist data in any kind of file
object: ``open``, ``gzip.GzipFile``, ``bz2.Bz2file`` and ``lzma.LzmaFile``. For
performance reason and usability, the new joblib version uses its own file
object ``BinaryZlibFile`` to compress pickle using zlib compression. Indeed,
``GzipFile`` could be seen as a good candidate as it's also based on zlib but the
format computes a crc for each chunk of compressed data, making it slower (we
noticed a performance drop of 15%).

.. note::
   There's also a small speed difference with dict/list objects between new/old
   joblib when using compression.
   The old version pickles the data inside a ``io.BytesIO`` buffer and then
   compress it in a row whereas the new version write "on the fly" compressed
   chunk of pickled data to the file.
   Because of this internal buffer the old implementation is not memory safe as it
   indeed copy the data in memory before compressing. The small speed difference
   was judged acceptable compared to this memory duplication.


Conclusion and future work
==========================


Memory copies were a ressource gap when caching on disk very large
numpy arrays, e.g arrays with a size close to the available RAM on the computer.
The solution was to use intensive buffering and a lot of hacking on top of
pickle and numpy. Unfortunately, this doesn't solve the poor performance with
big dictionaries or list compared to a ``cPickle`` base strategy.

Pickling numpy arrays using file handle is a first step toward pickling in
sockets. Then it will make broadcasting of data possible between computing units
on a network.

Another potential improvements is to make the supported list of compressors
extendable by allowing external project to register new ones. Some work has
already been started with LZO (using python-lzo) but LZ4 also seems to be an
interesting ones.

Thanks to `@lesteve <https://github.com/lesteve>`_,
`@ogrisel <https://github.com/ogrisel>`_ and
`@GaelVaroquaux <https://github.com/GaelVaroquaux>`_ for the valuable help,
reviews and support.

