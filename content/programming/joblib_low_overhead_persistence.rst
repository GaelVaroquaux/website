New Python compressed persistence in joblib
###############################################

:date: 2016-05-20
:tags: joblib, persistence, big data
:slug: New low-overhead persistence in joblib for big data
:authors: Alexandre Abadie & Gaël Varoquaux
:profile_image: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/110px-Python-logo-notext.svg.png
:summary: New persistence in joblib enables low-overhead storage of big data contained in arbitrary objects


Problem setting
=================

Joblib is a powerful Python package when working on so called **Big Data**, e.g
data that can consume up to the available RAM (several GB nowadays), generally
numpy arrays or arbitrary containers (list, dict) of numpy arrays. In order to
speedup Big Data processing and management, joblib uses a transparent disk
persistence which is based on functions ``dump`` and ``load``. The internal
mecanism relies on an advanded usage of `pickle
<https://docs.python.org/3/library/pickle.html>`__.

`Recent improvements <https://github.com/joblib/joblib/pull/260>`__ to joblib 
reduce vastly the overhead when persisting data.

Limitations of the old implementation
--------------------------------------

❶ Dumping/loading persisted data **with compression** was a memory hog,
mainly because of internal copies of data, limitating the maximum size
of usable data with compressed persistence:

.. image:: {filename}attachments/old_pickle_mem_profile.png
   :class: large

We see the increased memory usage during the calls to ``dump`` and
``load`` functions, profiled using the `memory_profiler package
<https://pypi.python.org/pypi/memory_profiler>`__ with this `gist
<https://gist.github.com/aabadie/7cba3385406d1cec7d3dd4407ba3f164>`__

|
❷ Another drawback was that large numpy arrays (>10MB) contained in an
arbitrary Python object were dumped in separate ``.npy`` file, increasing
the load on the file system [#]_:

.. code-block:: python
              
     >>> import numpy as np
     >>> import joblib # joblib version: 0.9.4
     >>> obj = [np.ones((5000, 5000)), np.random.random((5000, 5000))]
     
     # 3 files are generated:
     >>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
     ['/tmp/test.pkl', '/tmp/test.pkl_01.npy.z', '/tmp/test.pkl_02.npy.z']
     >>> joblib.load('/tmp/test.pkl')
     [array([[ 1.,  1., ...,  1.,  1.]],
      array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]



.. XXX: announce content of post earlier
   
    Let's now discover the new features and improvements that comes with
    version 0.10.0. After that, we'll compare speed and memory consumption with
    other libraries and discuss the results. Then we'll give some details about the
    new internal implementation.

|

What's new
==========

❶ **Memory usage is now stable**:

.. image:: {filename}attachments/new_pickle_mem_profile.png

❷ **All numpy arrays are persisted in a single file**:
 
.. code-block:: python

                >>> import numpy as np
                >>> import joblib # joblib version: 0.10.0
                >>> obj = [np.ones((5000, 5000)), np.random.random((5000, 5000))]
                
                # only 1 file is generated:
                >>> joblib.dump(obj, '/tmp/test.pkl', compress=True)
                ['/tmp/test.pkl']
                >>> joblib.load('/tmp/test.pkl')
                [array([[ 1.,  1., ...,  1.,  1.]],
                 array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]

❸ **Persistence can be done in a filehandle**

❹ **More compression formats are available**

|

.. topic:: Backward compatibility

    Existing joblib users can be reassured : the new version is **still
    compatible with pickles generated by older versions** (>= 0.8.4). You
    are encouraged to update (rebuild?) your cache if you want to take
    advantage of this new version.


Benchmarks: speed and memory consumption
=========================================

Joblib strives to have **minimum dependencies** (only numpy) and to
**be agnostic to the input data**. Hence the goals are to deal with any
kind of data while trying to **be as efficient as possible with numpy arrays**.

To illustrate the benefits and cost of the new persistence implementation, let's
now compare a real life use case
(`LFW dataset from scikit-learn <http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html>`_)
with different libraries:

* Joblib, with 2 different versions,
  0.9.4 and master (dev),
* Pickle
* Numpy

Each approach is tested with and without compression (the compression level and underlying method are the same: zlib, with a
compression level of 3 -- gzip is based on zlib with crc checks).

.. image:: {filename}attachments/persistence_lfw_bench.png
    :class: large

           
The four first lines use non compressed persistence strategies, the last
four use persistence with zlib/gzip strategies. Code to reproduce the
benchmarks is available on this `gist
<https://gist.github.com/aabadie/2ba94d28d68f19f87eb8916a2238a97c>`_.


**Speed**: the results between joblib 0.9.4 and 0.10.0 are
similar whereas **numpy and pickle are clearly slower than joblib** in both
compressed and non compressed cases.

**Memory consumption**: Without compression, old and
new joblib versions are the same; with compression, the old joblib version is
clearly worse than the new one.
Again **pickle and moreover numpy are clearly worse than joblib in terms of
memory consumption**. This can be explained by the fact that numpy relies on
pickle if the object is not a pure numpy array (a list or a dict with arrays for
example), so in this case it inherits the memory drawbacks from pickle. When
persisting pure numpy arrays (not tested here), numpy uses its internal save/load
functions which are quite efficient in terms of speed and memory consumption.

**Disk used**: results are as expected: non compressed files haves
the same size as the in-memory data; compressed files are smaller.

.. topic:: Caveat Emptor: performance is data-dependent

    Different data compress more or less easily. Results from the
    benchmarks, in particular speed and disk used, will vary depending on
    the data. Key considerations are:

    * **Fraction of data in arrays**: joblib is efficient if much of the
      data is contained in numpy arrays. The worst case scenario is
      something like a large dictionary of random numbers as keys and
      values.

    * **Entropy of the data**: an array fully of zeros will compress well
      and fast. A fully random array will compress slowly, and use a lot
      of disk. Real data is often somewhere in the middle.


Extra improvements in compressed persistence
=============================================

New compression formats
------------------------

Joblib can use new compression formats based on Python standard library modules:
**zlib, gzip, bz2, lzma and xz** (the last 2 are available for Python
greater than 3.3). **The compressor is
selected automatically when the file name has an explicit extension**:

.. code-block:: python
               
      >>> joblib.dump(obj, '/tmp/test.pkl.z')   # zlib
      ['/tmp/test.pkl.z']
      >>> joblib.dump(obj, '/tmp/test.pkl.gz')  # gzip
      ['/tmp/test.pkl.gz']
      >>> joblib.dump(obj, '/tmp/test.pkl.bz2')  # bz2
      ['/tmp/test.pkl.bz2']
      >>> joblib.dump(obj, '/tmp/test.pkl.lzma')  # lzma
      ['/tmp/test.pkl.lzma']
      >>> joblib.dump(obj, '/tmp/test.pkl.xz')  # xz
      ['/tmp/test.pkl.xz']

One can tune the compression level, setting the compressor explicitly:

.. code-block:: python
               
      >>> joblib.dump(obj, '/tmp/test.pkl.compressed', compress=('zlib', 6))
      ['/tmp/test.pkl.compressed']
      >>> joblib.dump(obj, '/tmp/test.compressed', compress=('lzma', 6))
      ['/tmp/test.pkl.compressed']

On loading, joblib uses the Magic number of the file to determine the
right decompression format, making compressed pickle load transparent:

.. code-block:: python
               
       >>> joblib.load('/tmp/test.compressed')
       [array([[ 1.,  1., ...,  1.,  1.]],
        array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]

An important thing is that the generated compressed files uses a **standard
compression file format**: for instance, regular command line tools (zip/unzip,
gzip/gunzip, bzip2, lzma, xv) can be used to compress/uncompress a pickled file
generated with joblib. Joblib will be able to load cache compressed with those
tools. 

Compressed persistence into a file handle
-----------------------------------------

Now that all numpy arrays are stored in a
single file, itself using standard compression formats, joblib can
persist in an open file handle:

.. code-block:: python
               
     >>> with open('/tmp/test.pkl', 'wb') as f:
     >>>    joblib.dump(obj, f)
     ['/tmp/test.pkl']
     >>> with open('/tmp/test.pkl', 'rb') as f:
     >>>    print(joblib.load(f))
     [array([[ 1.,  1., ...,  1.,  1.]],
      array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]

This also works with compression file object available in the standard library,
like ``gzip.GzipFile``, ``bz2.Bz2File`` or ``lzma.LzmaFile``:

.. code-block:: python

     >>> import gzip
     >>> with gzip.GzipFile('/tmp/test.pkl.gz', 'wb') as f:
     >>>    joblib.dump(data, f)
     ['/tmp/test.pkl.gz']
     >>> with gzip.GzipFile('/tmp/test.pkl.gz', 'rb') as f:
     >>>    print(joblib.load(f))
     [array([[ 1.,  1., ...,  1.,  1.]],
      array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]


Be sure that you use a decompressor matching the internal compression when
loading with the above method. If
unsure, simply use ``open``, joblib will **select the right decompressor**:


.. code-block:: python

     >>> with open('/tmp/test.pkl.gz', 'rb') as f:
     >>>     print(joblib.load(f))
     [array([[ 1.,  1., ...,  1.,  1.]],
      array([[ 0.47006195,  0.5436392 , ...,  0.1218267 ,  0.48592789]])]

.. topic:: Towards dumping to elaborate stores

    Working with file handles opens the door to **storing cache data in database blob or cloud
    storage such as Amazon S3, Amazon Glacier and Google Cloud Storage**
    (for instance via the Python package `boto
    <https://github.com/boto/boto>`_).

          
Implementation
====================

Historically, joblib relies on subclassing the Python
Pickler/Unpickler [#]_. This has been sligthly
refactored in the new version as follows:

* **Pickling an arbitrary object**: if an ``np.ndarray`` object is reached,
  instead of using the default pickling functions (__reduce__()), the joblib
  Pickler replaces in pickle stream the ndarray with a wrapper object containing
  all important array metadata (shape, dtype, flags). Then it writes the array
  content in the pickle file. **Note that this step breaks the pickle
  compatibility**.
* **Unpickling from a file**: when pickle reaches the array wrapper, as the
  object is already fully read in the pickle stream, the file handle is at the
  beginning of the array content. So at this point the Unpickler simply
  reconstruct an array based on the metadata contained in the wrapper and then
  fill the array buffer directly from the file. The object returned is the
  reconstructed array, the array wrapper being dropped.

This technique allows joblib to pickle all objects in a single file but also to
stay efficient in memory consumption during dump and load.

The other main change in the current persitence workflow concerns the
compression strategy. As the pickling refactoring opens the door
to file objects usage, joblib is now able to persist data in any kind of file
object: ``open``, ``gzip.GzipFile``, ``bz2.Bz2file`` and ``lzma.LzmaFile``. For
performance reason and usability, the new joblib version uses its own file
object ``BinaryZlibFile`` for zlib compression. Compared to 
``GzipFile``, it disables crc computation, which bring a performance gain of 15%.

.. topic:: Speed penalties of on-the-fly writes

   There's also a small speed difference with dict/list objects between new/old
   joblib when using compression.
   The old version pickles the data inside a ``io.BytesIO`` buffer and then
   compress it in a row whereas the new version write "on the fly" compressed
   chunk of pickled data to the file.
   Because of this internal buffer the old implementation is not memory safe as it
   indeed copy the data in memory before compressing. The small speed difference
   was judged acceptable compared to this memory duplication.


Conclusion and future work
==========================


Memory copies were a ressource gap when caching on disk very large
numpy arrays, e.g arrays with a size close to the available RAM on the computer.
The solution was to use intensive buffering and a lot of hacking on top of
pickle and numpy. Unfortunately, this doesn't solve the poor performance with
big dictionaries or list compared to a ``cPickle`` base strategy.

Pickling numpy arrays using file handle is a first step toward pickling in
sockets. Then it will make broadcasting of data possible between computing units
on a network.

Another potential improvements is to make the supported list of compressors
extendable by allowing external project to register new ones. Some work has
already been started with LZO (using python-lzo) but LZ4 also seems to be an
interesting ones.

The pull request was implemented by `@aabadie
<https://github.com/aabadie>`_. He thanks `@lesteve
<https://github.com/lesteve>`_, `@ogrisel <https://github.com/ogrisel>`_
and `@GaelVaroquaux <https://github.com/GaelVaroquaux>`_ for the valuable
help, reviews and support.

|

____

.. [#] The load created by multiple files on the filesystem is
   particularly detrimental for network filesystems, as it triggers
   multiple requests and isn't cache friendly.

.. [#] A drawback of subclassing the Python Pickler/Unpickler is that it
   is done for the pure-Python version, and not the "cPickle" version.
   The latter is much faster when dealing with a large number of Python
   objects. Once again, joblib is efficient when most of the data is
   represented as numpy arrays or subclasses.
   
